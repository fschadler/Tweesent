from tweepy import API
from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream
import time
import csv
import yweather
import pandas as pd
import numpy as np
import re
from textblob import TextBlob, Word, WordList
import wordcloud
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as sia
from googletrans import Translator

import twitter_credentials

# User Input: Defines the search term and amount of Tweets that shall be analyzed.

######  Add predefined # infront of user Input, but if User manually adds # remove it "If str contains # remove "#""
hash_tag_list = input("Enter keyword/hashtag: ")
num_terms = input("Enter how many Tweets should be included: ")


class TwitterClient():
    def __init__(self):
        self.auth = TwitterAuthenticator().authenticate_twitter_app()
        self.twitter_client = API(self.auth)

    def get_twitter_client_api(self):
        return self.twitter_client


# TWITTER AUTHENTICATOR
"""
Uses twitter_credentials.py to get access to API 
"""


class TwitterAuthenticator():

    def authenticate_twitter_app(self):
        auth = OAuthHandler(twitter_credentials.CONSUMER_KEY, twitter_credentials.CONSUMER_SECRET)
        auth.set_access_token(twitter_credentials.ACCESS_TOKEN, twitter_credentials.ACCESS_TOKEN_SECRET)
        return auth


# TWITTER STREAMER
class TwitterStreamer():
    """
    Class for streaming and processing live tweets.
    """

    def __init__(self):
        self.twitter_autenthicator = TwitterAuthenticator()

    def stream_tweets(self, fetched_tweets_filename, hash_tag_list):
        # This handles Twitter authentification and the connection to Twitter Streaming API
        listener = TwitterListener(fetched_tweets_filename)
        auth = self.twitter_authenticator.authenticate_twitter_app()
        stream = Stream(auth, listener)

        # Filter based on User Input
        stream.filter(track=hash_tag_list)

"""
# TWITTER STREAM LISTENER
class TwitterListener(StreamListener):
    """
    This is a basic listener that just prints received tweets to stdout.
    """

    def __init__(self, fetched_tweets_filename):
        self.fetched_tweets_filename = fetched_tweets_filename

    def on_data(self, data):
        try:
            print(data)
            with open(self.fetched_tweets_filename, 'a') as tf:
                tf.write(data)
            return True
        except BaseException as e:
            print("Error on_data %s" % str(e))
        return True

    def on_error(self, status):
        if status == 420:
            # Returning False on_data method in case rate limit occurs.
            return False
        print(status)

"""
class TweetAnalyzer():
    """
    Functionality for analyzing and categorizing content from tweets.
    """

    def clean_tweet(self, tweet):
        return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())

    #    def translate_tweet(self,tweet):
    #        language = TextBlob(self.clean_tweet(tweet))
    #        detected_language = language.detect_language()
    #        for language in self.clean_tweet(tweet):
    #            if language.detect_language != "en":
    #                return language.translate(from_lang=detected_language, to=u"en")

    def analyze_sentiment(self, tweet):
        analysis = TextBlob(self.clean_tweet(tweet))

        return analysis.sentiment.polarity

    def tweets_to_data_frame(self, tweets):
        df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['tweets'])

        df['id'] = np.array([tweet.id for tweet in tweets])  # Tweet Object
        df['len'] = np.array([len(tweet.text) for tweet in tweets])  # Tweet Object
        df['date'] = np.array([tweet.created_at for tweet in tweets])  # Tweet Object
        df['lang'] = np.array([tweet.lang for tweet in tweets])  # Tweet Object  ----> If translation is needed, we could use this instead of Vader/Blob to detect the language
        df['user_id'] = np.array([tweet.user.id for tweet in tweets])  # User Object
        df['user_screen_name'] = np.array([tweet.user.screen_name for tweet in tweets])  # User Object
        df['follower_count'] = np.array([tweet.user.followers_count for tweet in tweets])  # User Object
        df['location'] = np.array([tweet.user.location for tweet in tweets])  # User Object

        return df


if __name__ == '__main__':
    twitter_client = TwitterClient()
    tweet_analyzer = TweetAnalyzer()

    api = twitter_client.get_twitter_client_api()
    # Tweets filtered based on manual user input
    tweets = api.search(q=hash_tag_list, count=num_terms)
    df = tweet_analyzer.tweets_to_data_frame(tweets)

    # Adds Sentiment Score to dataframe
    df['sentiment'] = np.array([tweet_analyzer.analyze_sentiment(tweet) for tweet in df['tweets']])
    # Additional dataframe which only displays Tweets of users with 10.000 or more followers
    df_follower = df.loc[df['follower_count'] > 10000]

    # Prints all Tweets and Sentiment Score
    print(df.head(int(num_terms)))
    # Prints the average score of the loaded Tweets
    print("Average Sentiment Score of ", hash_tag_list, ", including ", num_terms, "Tweets is: ",
          sum(df["sentiment"]) / len(df["sentiment"]))
    # Prints only Tweets with more then 10000 followers
    print(df_follower)

def word_cloud(wd_list):
    stopwords = set(STOPWORDS)
    all_words = ' '.join([text for text in wd_list])
    wordcloud = WordCloud(background_color='white',stopwords=stopwords,width=1600,height=800,random_state=21,colormap='jet',max_words=50,max_font_size=200).generate(all_words)
    plt.figure(figsize=(12, 10))
    plt.axis('off')
    plt.imshow(wordcloud, interpolation="bilinear");
    
#stream listener
"""
Give a file name (any name with .csv at the end and filter words in form of a list e.g. ["trump","wall"] and time limit in seconds to get a csv file --> Live stream
"""
def twitter_stream_listener(file_name,filter_track,follow=None,locations=None,languages=None,time_limit=20):
    class CustomStreamListener(tweepy.StreamListener):
        def __init__(self, time_limit):
            self.start_time = time.time()
            self.limit = time_limit
            # self.saveFile = open('abcd.json', 'a')
            super(CustomStreamListener, self).__init__()
        def on_status(self, status):
            if (time.time() - self.start_time) < self.limit:
                print(".", end="")
                # Writing status data
                with open(file_name, 'a') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        status.author.screen_name, status.created_at,
                        status.text
                    ])
            else:
                print("\n\n[INFO] Closing file and ending streaming")
                return False
        def on_error(self, status_code):
            if status_code == 420:
                print('Encountered error code 420. Disconnecting the stream')
                # returning False in on_data disconnects the stream
                return False
            else:
                print('Encountered error with status code: {}'.format(
                    status_code))
                return True  # Don't kill the stream
        def on_timeout(self):
            print('Timeout...')
            return True  # Don't kill the stream
    # Writing csv titles
    print('\n[INFO] Open file: [{}] and starting {} seconds of streaming for {}\n'.format(file_name, time_limit, filter_track))
    with open(file_name, 'w') as f:
        writer = csv.writer(f)
        writer.writerow(['author', 'date', 'text'])
    streamingAPI = tweepy.streaming.Stream(
        auth, CustomStreamListener(time_limit=time_limit))
    streamingAPI.filter(track=filter_track,follow=follow,locations=locations,languages=languages)
    f.close()

def find_woeid(location): #woe-id = where on earth id --> Input bspw. "New York" 
    wc = yweather.Client()
    woeid = wc.fetch_woeid(location)
    print(woeid)
    
def trend_search(woeid): #10 Trends based on woe-id z.B. 2459115 --> "New York"
    try:
        trends_results = api.trends_place(woeid)
        for trend in trends_results[0]["trends"][:10]:
            print(trend["name"])
    except tweepy.error.TweepError:
        print("There are no trending topics for your location.")    
